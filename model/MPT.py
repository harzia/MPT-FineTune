import torch
from weaver.utils.logger import _logger

from model.MoETransformer import MoeParticleTransformer


class MoeParticleTransformerWrapper(torch.nn.Module):
    def __init__(self, **kwargs) -> None:
        super().__init__()
        self.mod = MoeParticleTransformer(**kwargs)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'mod.cls_token', }

    def forward(self, points, features, lorentz_vectors, mask):
        return self.mod(features, v=lorentz_vectors, mask=mask)


def get_model(data_config, **kwargs):
    
    cfg = dict(
        input_dim=len(data_config.input_dicts['pf_features']),
        num_classes=len(data_config.label_value),
        # network configurations
        pair_input_dim=4,
        use_pre_activation_pair=False,
        embed_dims=[128, 512, 128],
        pair_embed_dims=[64, 64, 64],
        num_heads=8,
        num_layers=8,
        num_cls_layers=2,
        ffn_ratio=4,
        block_params=None,
        cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},
        fc_params=[],
        activation='gelu',
        # misc
        trim=True,
        for_inference=False,
        # MoE settings
        moe_num_experts=8,
        moe_top_k=2,
        moe_capacity_factor=1.5,
        moe_aux_loss_coef=0.01,
        moe_router_jitter=0.01,
    )
    # Allow overrides via kwargs and environment variables (from ConfigMap)
    cfg.update(**kwargs)
    _logger.info('Model config: %s' % str(cfg))

    model = MoeParticleTransformerWrapper(**cfg)
    
    model_info = {
        'input_names': list(data_config.input_names),
        'input_shapes': {k: ((1,) + s[1:]) for k, s in data_config.input_shapes.items()},
        'output_names': ['softmax'],
        'dynamic_axes': {**{k: {0: 'N', 2: 'n_' + k.split('_')[0]} for k in data_config.input_names}, **{'softmax': {0: 'N'}}},
    }
    return model, model_info


def get_loss(data_config, **kwargs):
    class CombinedLoss(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.ce_loss = torch.nn.CrossEntropyLoss()

        def forward(self, output, target, model=None):
            ce_loss = self.ce_loss(output, target)

            if model is None:
                return ce_loss

            # Handle DataParallel/DDP wrappers
            if hasattr(model, 'module'):
                model = model.module
            
            if hasattr(model, 'mod') and hasattr(model.mod, '_moe_aux_loss'):
                ce_loss = ce_loss + model.mod._moe_aux_loss
            else:
                _logger.info("Warning: MoE Aux loss not found on model!")
                
            return ce_loss

    return CombinedLoss()